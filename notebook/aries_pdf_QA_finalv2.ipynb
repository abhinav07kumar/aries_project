{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Xrq_JteLTyqz",
        "D4xajFLWtV8j",
        "jpMDALw3Ue7n",
        "SE1A0UBxWL4R",
        "zcYrB593oTSc",
        "tv7Y6-vFpAS5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Installing dependencies"
      ],
      "metadata": {
        "id": "Xrq_JteLTyqz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmiyzmJzJWVz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages, good for effecient and fast finetuning of large models (optional)\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install pypdf\n",
        "!pip install fitz\n",
        "!pip install pymupdf\n",
        "!pip install unstructured python-magic\n",
        "!pip install faiss-gpu\n",
        "!pip install transformers torch huggingface_hub\n",
        "!pip install python-dotenv==1.0.0 streamlit==1.22.0 tiktoken==0.4.0\n",
        "!pip install protobuf~=3.20\n",
        "!pip install sentence-transformers\n",
        "!pip install rich"
      ],
      "metadata": {
        "id": "ALk3gbXlT9Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Necessary imports"
      ],
      "metadata": {
        "id": "D4xajFLWtV8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "fBc7o0ShM_Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "import faiss\n",
        "import os\n",
        "import pandas as pd\n",
        "import magic\n",
        "import os\n",
        "import nltk\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import PromptTemplate\n",
        "from google.colab import userdata\n",
        "from transformers import TextStreamer\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "from rich.table import Table\n",
        "from rich.panel import Panel\n",
        "from rich.text import Text\n"
      ],
      "metadata": {
        "id": "_oo7DwdJy3bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Mount Drive"
      ],
      "metadata": {
        "id": "jpMDALw3Ue7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t4NFGTSkUeQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Base model loading in quantized in 4-bit"
      ],
      "metadata": {
        "id": "SE1A0UBxWL4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length =  4096 #You can set this upto {8192- (max number of output tokens you want)}\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "TRB2_aLdy5pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing Model for Low Rank Adaptation"
      ],
      "metadata": {
        "id": "UyAKDa7MWhZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "aZUNHBawy9AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #####System Prompt - To use later for RAG with the finetuned LLM\n",
        "alpaca_prompt = \"\"\"Below is a query asked by a user , paired with additonal data that provides further context. Write a response that appropriately answers the query.\n",
        "You must answer only from the given data and not make up anything. Be as detailed as possible. If the answer is not present in the data, just say \"I don't know\".\n",
        "\n",
        "### Query:\n",
        "{}\n",
        "\n",
        "### Data:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "ev8Kl8gWLhrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN #Adding EOS token as special token to vocabulary\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ],
      "metadata": {
        "id": "2chPRgA5MJxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #####Loading the dataset (can be any of your choice but I chose a standard one)\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "PtefNGl2MJaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "IRER7gDNJYEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Initializing training parameters using LoRA"
      ],
      "metadata": {
        "id": "QqbqYSzRW4nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 100,  # 1 epoch is about 6000 steps for alpaca_cleaned dataset (50k examples)\n",
        "        # num_train_epochs = 6,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "Z4VpeuGvMhF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #####Memory statistics\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "gpVm6PAIN4BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LoRA enabled Fine-Tuning\n",
        "\n"
      ],
      "metadata": {
        "id": "VSC1fX7TXZs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "A7xEd_awN-FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #####Final memory and time statistics after training\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "So5B33AMOcIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### Test Inference\n",
        "\n",
        "# alpaca_prompt = from above\n",
        "FastLanguageModel.for_inference(model) #Testing\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Say the opposite of the data porvided.\", # query\n",
        "        \"Hello\", # data\n",
        "        \"\", # output - leave this blank\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128) # As Llama3_8b has a total context window of 8k tokens , you can adjust max_new_tokens with the upper limit of 8000-input tokens"
      ],
      "metadata": {
        "id": "Wh4wy7SnWkms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save the Model (for effeciency this just saves the LoRA adapters)"
      ],
      "metadata": {
        "id": "gcjHltMmoDi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/aries_llama3_8b/aries_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/aries_llama3_8b/aries_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "id": "GmzaTVGYWlFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Load the Model from whatever path you saved it to"
      ],
      "metadata": {
        "id": "JZqd1EIzoHXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 4096 #You can set this upto {8000- (max number of output tokens you want)} to be under the context window limit\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "if True:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"/content/drive/MyDrive/aries_llama3_8b/aries_model\", # Loading the model using the saved path (here I saved in my drive but you can upload too)\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit, # Again, loading in 4-bit to preserve memory and enhance speed\n",
        "\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) #Initialize it for inference"
      ],
      "metadata": {
        "id": "wI306Vp1WrIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #####System Prompt\n",
        "alpaca_prompt = \"\"\"Below is a query asked by a user , paired with additonal data that provides further context. Write a response that appropriately answers the query.\n",
        "You must answer only from the given data and not make up anything. Be as detailed as possible. If the answer is not present in the data, just say \"I don't know\".\n",
        "\n",
        "### Query:\n",
        "{}\n",
        "\n",
        "### Data:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "geWhc_niSm-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN #Adding EOS token as special token to vocabulary\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ],
      "metadata": {
        "id": "xHCKgrq3Sm-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Document Loading and preprocessing"
      ],
      "metadata": {
        "id": "zcYrB593oTSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "file_path = '/content/drive/MyDrive/mml-book.pdf' #The path to the document you need QA facilities for\n",
        "loader = PyMuPDFLoader(file_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# You can use this code if you have a full directory of pdfs instead of a single one\n",
        "\n",
        "# pdf_directory = \"/content/drive/MyDrive/\"\n",
        "# documents = []\n",
        "# for root, dirs, files in os.walk(pdf_directory):\n",
        "#     for file in files:\n",
        "#         if file.endswith(\".pdf\"):\n",
        "#             file_path = os.path.join(root, file)\n",
        "#             loader = PyMuPDFLoader(file_path)\n",
        "#             documents.extend(loader.load())"
      ],
      "metadata": {
        "id": "B1uZ8u1Sb1Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of documents loaded\n",
        "print(f\"Number of documents loaded: {len(documents)}\")"
      ],
      "metadata": {
        "id": "kgWv7DZBbevr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
        "\n",
        "# Split the documents into chunks for ease of search and incorporation into RAG later\n",
        "split_documents = text_splitter.split_documents(documents)\n",
        "print(f\"Number of chunks: {len(split_documents)}\")"
      ],
      "metadata": {
        "id": "sWkaZwqPbhEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### Embedding function to convert the text into vectors\n",
        "\n",
        "#You can optionally specify model name or even use a different embedding function, it works the same way\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "nUtv1c95brYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creation of vector database to store the embeddings and enable similarity search"
      ],
      "metadata": {
        "id": "tv7Y6-vFpAS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory_faiss = \"/content/drive/MyDrive/FAISS_aries\""
      ],
      "metadata": {
        "id": "kQZKMdRpMbTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### Create and save the database of you embedded document chunks\n",
        "\n",
        "vector_db_faiss = FAISS.from_documents(documents = split_documents, embedding = embeddings)\n",
        "vector_db_faiss.save_local(save_directory_faiss) #You can save your vector database to the apth specified above"
      ],
      "metadata": {
        "id": "NUoQwXV3blNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### Or load it if you already have saved one and do not wish to create again\n",
        "vector_db_faiss = FAISS.load_local(save_directory_faiss, embeddings, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "LJQJrVIjMioh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Searching the vector database from a user query (for demonstration and insight)"
      ],
      "metadata": {
        "id": "5cELTbcMpm03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = input(\"Please enter your query: \")"
      ],
      "metadata": {
        "id": "uRT0OEgfprhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity search with score returns a list of bith the chunks and their corresponding score\n",
        "\n",
        "docs = vector_db_faiss.similarity_search_with_score(user_input, k = 6) # k is the number of similar chunks you want to fetch to pass into your model , do not keep it too high or you will exceed your context window limit"
      ],
      "metadata": {
        "id": "2u8ofh9JkhFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docs"
      ],
      "metadata": {
        "id": "AbaKk4A-QCvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting scores and documents separately (documents also contain metadata which I have extracted separartely later)\n",
        "similar_documents = []\n",
        "scores = []\n",
        "for doc, score in docs:\n",
        "  similar_documents.append(doc)\n",
        "  scores.append(score)"
      ],
      "metadata": {
        "id": "Oxb4AmFKQZhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similar_documents"
      ],
      "metadata": {
        "id": "10mx2Hi8QjpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract page contents and metadata separately\n",
        "page_contents = [doc.page_content for doc in similar_documents]\n",
        "metadata = [doc.metadata for doc in similar_documents]\n",
        "\n",
        "print(\"Page Contents:\")\n",
        "for i, content in enumerate(page_contents):\n",
        "    print(f\"Document {i+1} Content:\\n{content}\\n\")\n",
        "\n",
        "print(\"Metadata:\")\n",
        "for i, meta in enumerate(metadata):\n",
        "    print(f\"Document {i+1} Metadata:\\n{meta}\\n\")"
      ],
      "metadata": {
        "id": "w1fvxEgPRNQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# page_contents"
      ],
      "metadata": {
        "id": "H5o13OK-RfLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Inference and response by the model"
      ],
      "metadata": {
        "id": "tRub6Bj1rTNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_response(text):\n",
        "    response_marker = \"### Response:\"\n",
        "    start_index = text.find(response_marker)\n",
        "\n",
        "    if start_index == -1:\n",
        "        return \"Response marker not found\"\n",
        "\n",
        "    # Move the start_index to the position right after the marker\n",
        "    start_index += len(response_marker)\n",
        "\n",
        "    # Extract the response text\n",
        "    response_text = text[start_index:].strip()\n",
        "\n",
        "    return response_text\n",
        "\n",
        "# Function to save the conversation log to a text file\n",
        "def save_conversation_log(conversation_log, filename=\"conversation_log.txt\"):\n",
        "    with open(filename, \"w\") as file:\n",
        "        for entry in conversation_log:\n",
        "            file.write(entry + \"\\n\")\n",
        "            file.write(\"\\n\" + \"-\"*80 + \"\\n\")  # Add a separator between entries"
      ],
      "metadata": {
        "id": "p0_B2htG_PKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize conversation log and rich console\n",
        "conversation_log = []\n",
        "console = Console()\n",
        "\n",
        "console.print(\"[bold green]Welcome! Type your query below. Type 'exit' to stop the conversation.[/bold green]\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Please enter your query: \")\n",
        "\n",
        "    # Check if the user wants to exit the loop\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Add user query to the conversation log\n",
        "    conversation_log.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Perform similarity search with score\n",
        "    docs = vector_db_faiss.similarity_search_with_score(user_input, k=6)  # Adjust k as needed\n",
        "\n",
        "    # Extracting scores and documents separately\n",
        "    similar_documents = []\n",
        "    scores = []\n",
        "    for doc, score in docs:\n",
        "        similar_documents.append(doc)\n",
        "        scores.append(score)\n",
        "\n",
        "    # Extract page contents and metadata separately\n",
        "    page_contents = [doc.page_content for doc in similar_documents]\n",
        "    metadata = [doc.metadata for doc in similar_documents]\n",
        "\n",
        "    # Display the page contents, metadata, and scores using rich Table\n",
        "    table = Table(title=\"Retrieved Documents\")\n",
        "    table.add_column(\"Page Content\", justify=\"left\", style=\"cyan\", no_wrap=False)\n",
        "    table.add_column(\"Metadata\", justify=\"left\", style=\"magenta\")\n",
        "    table.add_column(\"Score\", justify=\"left\", style=\"green\")\n",
        "\n",
        "    for content, meta, score in zip(page_contents, metadata, scores):\n",
        "        table.add_row(content, str(meta), str(score))\n",
        "\n",
        "    console.print(table)\n",
        "\n",
        "    # Prepare inputs for the model\n",
        "    inputs = tokenizer(\n",
        "        [\n",
        "            alpaca_prompt.format(\n",
        "                user_input,  # query\n",
        "                page_contents,  # data\n",
        "                \"\",  # output - leave this blank\n",
        "            )\n",
        "        ],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate response\n",
        "    # text_streamer = TextStreamer(tokenizer)\n",
        "    # outputs = model.generate(**inputs, streamer=text_streamer, max_new_tokens=4096)\n",
        "    # response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(**inputs, max_new_tokens=4096, pad_token_id=tokenizer.eos_token_id, use_cache=True)\n",
        "    # Extract response\n",
        "    response = extract_response(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "    # Add model response to the conversation log\n",
        "    conversation_log.append(f\"Model: {response}\")\n",
        "\n",
        "    # Display the user's query and the model's final response using rich Panel and Text\n",
        "    query_text = Text(f\"Query:\\n\\n{user_input}\", justify=\"left\")\n",
        "    response_text = Text(f\"Response:\\n\\n{response}\", justify=\"left\")\n",
        "\n",
        "    console.print(Panel(query_text, title=\"User Query\", subtitle_align=\"left\"))\n",
        "    console.print(Panel(response_text, title=\"Model Response\", subtitle_align=\"left\"))\n",
        "\n",
        "    # Log retrieved documents, metadata, and scores neatly\n",
        "    conversation_log.append(\"Retrieved Documents:\\n\")\n",
        "    for i, (content, meta, score) in enumerate(zip(page_contents, metadata, scores)):\n",
        "        conversation_log.append(f\"Document {i+1} Metadata:\\n{meta}\")\n",
        "        conversation_log.append(f\"Document {i+1} Content:\\n{content}\")\n",
        "        conversation_log.append(f\"Document {i+1} Score:\\n{score}\")\n",
        "        conversation_log.append(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "# Save the conversation log to a text file\n",
        "save_conversation_log(conversation_log)\n",
        "console.print(\"[bold green]Conversation saved to conversation_log.txt[/bold green]\")\n"
      ],
      "metadata": {
        "id": "La02GJAFobV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}