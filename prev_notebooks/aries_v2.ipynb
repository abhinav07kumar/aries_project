{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIBhaZGY8epd"
      },
      "source": [
        "#### Incorporating Final RAG components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3kEG2whENRf"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im4mEUpbJ82M"
      },
      "outputs": [],
      "source": [
        "save_directory_faiss = \"/content/drive/MyDrive/FAISS\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-UWAQ6fEvWk"
      },
      "outputs": [],
      "source": [
        "# Using FAISS vector database - use for faster search\n",
        "vector_db_faiss = FAISS.from_documents(documents = split_documents, embedding = embeddings)\n",
        "vector_db_faiss.save_local(save_directory_faiss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJTpsOpZFCTA"
      },
      "outputs": [],
      "source": [
        "vector_db_faiss2 = FAISS.load_local(save_directory_faiss, embeddings, allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p38KEDTcQrpW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vF-J3bqT26w"
      },
      "outputs": [],
      "source": [
        "retriever_faiss = vector_db_faiss.as_retriever(search_kwargs={\"k\": 10})\n",
        "retriever_faiss_mmr = vector_db_faiss.as_retriever(search_type = \"mmr\", fetch_k =50, search_kwargs={\"k\": 5, \"lambda_mult\": 0.5})\n",
        "retriever_faiss_sst = vector_db_faiss.as_retriever(search_type = \"similarity_score_threshold\", search_kwargs={\"k\": 5, \"score_threshold\": 0.8})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lecsocpF7qh"
      },
      "outputs": [],
      "source": [
        "# persist_directory = \"/content/drive/MyDrive/data/ChromaDB\"\n",
        "# # Using ChromaDB vector data base - Use if data base has a  very large size\n",
        "# vector_db_chroma = Chroma.from_documents(documents=split_documents, embedding=embeddings, persist_directory=persist_directory,)\n",
        "# vector_db_chromas = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "# retriever_chroma = vector_db_chroma.as_retriever(search_kwargs={\"k\": 5})\n",
        "# retriever_chroma_mmr = vector_db_chroma.as_retriever(search_type = \"mmr\", search_kwargs={\"k\": 5, \"lambda_mult\": 0.5})\n",
        "# retriever_chroma_sst = vector_db_chroma.as_retriever(search_type = \"similarity_score_threshold\", search_kwargs={\"k\": 5, \"score_threshold\": 0.8})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGmKHje7AxcH"
      },
      "outputs": [],
      "source": [
        "# Retrival using MultiQuery\n",
        "llm_retriever = ChatOpenAI(temperature=0)\n",
        "\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=retriever_faiss_mmr, llm=llm_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfwJI8PuRgHx"
      },
      "outputs": [],
      "source": [
        "results = vector_db_faiss2.similarity_search_with_score(\"Shloka\", k = 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmuyOLy-w2zn"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "scores = []\n",
        "for doc, score in results:\n",
        "    documents.append(doc)\n",
        "    scores.append(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PxEh2VKx4a2"
      },
      "outputs": [],
      "source": [
        "documents[0].metadata[\"source\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2ZqJSrsCX1f"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mA1RtTrFTbI"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.9, model='gpt-4o')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTM9SygqFTbI"
      },
      "outputs": [],
      "source": [
        "# Defining various prompt templates for different tasks\n",
        "\n",
        "\n",
        "#System Roles\n",
        "\n",
        "prompt_compilation = PromptTemplate(\n",
        "    input_variables=[\"data\"],\n",
        "    template=''' You will be given some data below, and a topic by the user. You have to properly and comprehensively compile the data to be relevant to the topic and explain it's concepts clearly from only the given data. Do not make up information.\n",
        "     {data}  '''\n",
        "\n",
        ")\n",
        "\n",
        "prompt_story_generation = PromptTemplate(\n",
        "    input_variables=[\"data\"],\n",
        "    template=''' You will be given some data below, and a topic by the user. You have to properly and comprehensively make a fun kids' story to explain the concepts relevant to the user's query in the datain simple terms so that kids can understand . Do not make up information.\n",
        "     {data}  '''\n",
        "\n",
        ")\n",
        "\n",
        "prompt_poem_generation = PromptTemplate(\n",
        "    input_variables=[\"data\"],\n",
        "    template=''' You will be given some data below, and a topic by the user. You have to properly and comprehensively make a fun kids' poem to explain the concepts relevant to the user's query in the data, in simple terms so that kids can understand. Do not make up information.\n",
        "     {data}  '''\n",
        "\n",
        ")\n",
        "\n",
        "prompt_image_generation = PromptTemplate(\n",
        "    input_variables=[\"data\"],\n",
        "    template=''' You will be given some story or poem below, and a topic by the user. Generate a detailed prompt, under 500 characters of length, to generate an image based on story/poem and the topic provided by the user.\n",
        "     {data}  '''\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "#Human Roles\n",
        "\n",
        "human_template_compilation = \"{topic}\"\n",
        "human_template_story_generation = \"{topic}\"\n",
        "human_template_poem_generation = \"{topic}\"\n",
        "human_template_image_generation = \"{topic}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4uN8stqFTbJ"
      },
      "outputs": [],
      "source": [
        "system_message_prompt_1 = SystemMessagePromptTemplate(prompt=prompt_compilation)\n",
        "human_message_prompt_1 = HumanMessagePromptTemplate.from_template(human_template_compilation)\n",
        "chat_prompt_compilation = ChatPromptTemplate.from_messages([system_message_prompt_1, human_message_prompt_1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEcTMAB978Fw"
      },
      "outputs": [],
      "source": [
        "system_message_prompt_2 = SystemMessagePromptTemplate(prompt=prompt_story_generation)\n",
        "human_message_prompt_2 = HumanMessagePromptTemplate.from_template(human_template_story_generation)\n",
        "chat_prompt_story_generation = ChatPromptTemplate.from_messages([system_message_prompt_2, human_message_prompt_2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukZibw5g9QsB"
      },
      "outputs": [],
      "source": [
        "system_message_prompt_3 = SystemMessagePromptTemplate(prompt=prompt_poem_generation)\n",
        "human_message_prompt_3 = HumanMessagePromptTemplate.from_template(human_template_poem_generation)\n",
        "chat_prompt_poem_generation = ChatPromptTemplate.from_messages([system_message_prompt_3, human_message_prompt_3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjfGk3Cy9RJK"
      },
      "outputs": [],
      "source": [
        "system_message_prompt_4 = SystemMessagePromptTemplate(prompt=prompt_image_generation)\n",
        "human_message_prompt_4 = HumanMessagePromptTemplate.from_template(human_template_image_generation)\n",
        "chat_prompt_image_generation = ChatPromptTemplate.from_messages([system_message_prompt_4, human_message_prompt_4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3ytGpb4CrQz"
      },
      "outputs": [],
      "source": [
        "llm_compressor = ChatOpenAI(temperature=0, model='gpt-4')\n",
        "\n",
        "compressor = LLMChainExtractor.from_llm(llm_compressor)\n",
        "# compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,\n",
        "#                                                        base_retriever=vectordb2.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":8, \"lambda_mult\": 0.8}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8o6MSOrjj6M"
      },
      "outputs": [],
      "source": [
        "i = 1\n",
        "processes = [\"compilation\", \"story_generation\", \"poem_generation\", \"image_generation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r1RDxJpGSaK"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_query = input(\"Your query: \")\n",
        "for process in processes:\n",
        "  if process == \"compilation\":\n",
        "    print(\"Fetching relevant documents.... \\n\\n\")\n",
        "    unique_docs_multi_query = retriever_from_llm.get_relevant_documents(query=input_query)\n",
        "    print(\"Displaying relevant document sources... \\n\\n\")\n",
        "    for i in range(len(unique_docs_multi_query)):\n",
        "      print(unique_docs_multi_query[i].metadata)\n",
        "    print(\"Compressing chunks for information density... \\n\\n\")\n",
        "    input_data = compressor.compress_documents(unique_docs_multi_query, query=input_query)\n",
        "    if (len(input_data)<2):\n",
        "      input_data = unique_docs_multi_query\n",
        "    print(\"\\n\")\n",
        "    print(input_data)\n",
        "    print(\"\\n\")\n",
        "    chat_prompt_with_values = chat_prompt_compilation.format_prompt(topic=input_query, data=input_data)\n",
        "    messages_compilation = chat_prompt_with_values.to_messages()\n",
        "    print(\"\\n\")\n",
        "    print(\"Compiling your data... \\n\\n\")\n",
        "    response = llm(messages_compilation)\n",
        "    print(response.content)\n",
        "    messages_compilation.append(AIMessage(content=response.content))\n",
        "    print(\"\\n\\n\\n\\n\")\n",
        "\n",
        "  elif process == \"story_generation\":\n",
        "      print(\"generating story... \\n\")\n",
        "\n",
        "  elif process == \"poem_generation\":\n",
        "      print(\"hello\")\n",
        "\n",
        "  elif process == \"image_generation\":\n",
        "      print(\"hello\")\n",
        "\n",
        "\n",
        "# else:\n",
        "#   # appending logic\n",
        "#   for process in processes:\n",
        "#     if process == \"compilation\":\n",
        "#         print(\"Fetching relevant documents.... \\n\\n\")\n",
        "#         unique_docs_multi_query = retriever_from_llm.get_relevant_documents(query=input_query)\n",
        "#         print(\"Displaying relevant document sources... \\n\\n\")\n",
        "#         for i in range(len(unique_docs_multi_query)):\n",
        "#           print(unique_docs_multi_query[i].metadata)\n",
        "#         print(\"Compressing chunks for information density... \\n\\n\")\n",
        "#         input_data = compressor.compress_documents(unique_docs_multi_query, query=input_query)\n",
        "#         if (len(input_data)<2):\n",
        "#           input_data = unique_docs_multi_query\n",
        "#         print(input_data)\n",
        "#         system_prompt_with_values = system_message_prompt.format(data = input_data)\n",
        "#         human_prompt_with_values = human_message_prompt.format(topic = input_query)\n",
        "#         messages_compilation.append(SystemMessage(content = system_prompt_with_values.content))\n",
        "#         messages_compilation.append(HumanMessage(content = human_prompt_with_values.content))\n",
        "#         print(\"Compiling your response... \\n\\n\")\n",
        "#         response = llm(messages_compilation)\n",
        "#         print(response.content)\n",
        "#         messages_compilation.append(AIMessage(content=response.content))\n",
        "#         print(\"\\n\\n\\n\\n\")\n",
        "\n",
        "#     elif process == \"story_generation\":\n",
        "#         print(\"hello\")\n",
        "#     elif process == \"poem_generation\":\n",
        "#         print(\"hello\")\n",
        "#     elif process == \"image_generation\":\n",
        "#         print(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Aw3JVEAeupf"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3cNHQTcjo5Q"
      },
      "outputs": [],
      "source": [
        "len(messages_compilation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xbRVCNVpjLw"
      },
      "outputs": [],
      "source": [
        "messages_compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV-tK6rZhzLN"
      },
      "outputs": [],
      "source": [
        "messages_compilation[0].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iccP3EXgibm0"
      },
      "outputs": [],
      "source": [
        "list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY9ZGaXYiYb_"
      },
      "outputs": [],
      "source": [
        "for i in range(len(messages_compilation)):\n",
        "  list.append(messages_compilation[i].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkQJ0FKviiAv"
      },
      "outputs": [],
      "source": [
        "list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAhtioll_8bJ"
      },
      "outputs": [],
      "source": [
        "# Define file paths\n",
        "system_messages_file = './responses/system/system_messages.txt'\n",
        "human_messages_file = './responses/human/human_messages.txt'\n",
        "ai_messages_file = './responses/ai/ai_messages.txt'\n",
        "\n",
        "# Initialize counters for each type of message\n",
        "system_index = 1\n",
        "human_index = 1\n",
        "ai_index = 1\n",
        "\n",
        "# Open files for writing\n",
        "with open(system_messages_file, 'w') as sys_file, open(human_messages_file, 'w') as hum_file, open(ai_messages_file, 'w') as ai_file:\n",
        "    # Iterate through the messages list\n",
        "    for message in messages:\n",
        "        # Check the type of the message and write to the corresponding file\n",
        "        if isinstance(message, SystemMessage):\n",
        "            sys_file.write(f\"SystemMessage index {system_index}:\\n{message.content}\\n\\n\")\n",
        "            system_index += 1\n",
        "        elif isinstance(message, HumanMessage):\n",
        "            hum_file.write(f\"HumanMessage index {human_index}:\\n{message.content}\\n\\n\")\n",
        "            human_index += 1\n",
        "        elif isinstance(message, AIMessage):\n",
        "            ai_file.write(f\"AIMessage index {ai_index}:\\n{message.content}\\n\\n\")\n",
        "            ai_index += 1\n",
        "\n",
        "print(\"Messages have been written to their respective files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8ts7hkpAHME"
      },
      "source": [
        "### **Code for various methods used for integration into RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SZ_IHrNAUTh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFDdRICw3y9u"
      },
      "source": [
        "#### Base RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCtXcIJ8h3yt"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.9, max_tokens=2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9dIRCZnknG_"
      },
      "outputs": [],
      "source": [
        "vector_db = FAISS.from_documents(documents = split_documents, embedding = embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVZVkCkukrEy"
      },
      "outputs": [],
      "source": [
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 10})\n",
        "q = input(\"Please ask your query : \")\n",
        "# should fetch  n relevant documents\n",
        "rdocs = retriever.get_relevant_documents(q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4CgdRzElA2R"
      },
      "outputs": [],
      "source": [
        "print(type(rdocs))\n",
        "print(len(rdocs))\n",
        "rdocs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7QjwxzBlNAS"
      },
      "outputs": [],
      "source": [
        "#design prompt template\n",
        "from langchain import PromptTemplate\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end, only using the data you are presented. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as detailed as possible. Always say \"Namaskaram\" at the end of the answer.\n",
        "context = {context}\n",
        "question = {question}\n",
        "Answer: \"\"\"\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_anmcqAlf6V"
      },
      "outputs": [],
      "source": [
        "# Modify the PromptTemplate to include context and question printing\n",
        "class CustomPromptTemplate(PromptTemplate):\n",
        "    def format_prompt(self, **kwargs):\n",
        "        context = kwargs.get(\"context\")\n",
        "        question = kwargs.get(\"question\")\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Context: {context}\")\n",
        "\n",
        "        return super().format_prompt(**kwargs)\n",
        "\n",
        "PROMPT = CustomPromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# Initialize the chain with the custom prompt template\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    input_key=\"query\",\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDpksX3PeGoa"
      },
      "outputs": [],
      "source": [
        "result = chain.invoke(q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCyY06n8lnXm"
      },
      "outputs": [],
      "source": [
        "print(result['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZafbIwmap94E"
      },
      "outputs": [],
      "source": [
        "len(result['source_documents'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PWh3SDFznH2"
      },
      "source": [
        "#### Multi Query Advanced Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XTO6wKLlzXo9"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al7MKf4tynNa"
      },
      "outputs": [],
      "source": [
        "vectordb2 = Chroma.from_documents(documents=split_documents, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwlcmj73z6QP"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "euKgLHa-z8Mv"
      },
      "outputs": [],
      "source": [
        "question = \"What are arteries and what is their difference with the pulmonary artery\"\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb2.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":8}), llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Hwrcgu_E0MNH"
      },
      "outputs": [],
      "source": [
        "unique_docs = retriever_from_llm.get_relevant_documents(query=question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "A_BYKXrx0S8r"
      },
      "outputs": [],
      "source": [
        "len(unique_docs)\n",
        "unique_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3kuWR1O0VnQ"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-sBfRgB6BYT"
      },
      "outputs": [],
      "source": [
        "text=PROMPT.format_prompt(\n",
        "    context=unique_docs,\n",
        "    question=question\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cvXpOX5rD7Gv"
      },
      "outputs": [],
      "source": [
        "unique_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YzgHFyQh0k_4"
      },
      "outputs": [],
      "source": [
        "llm.predict(text=PROMPT.format_prompt(\n",
        "    context=unique_docs,\n",
        "    question=question\n",
        ").text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0fN5ItQ0z8a"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0, model='gpt-4')\n",
        "\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,\n",
        "                                                       base_retriever=vectordb2.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":8, \"lambda_mult\": 0.8}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj0NlSxF1dcI"
      },
      "outputs": [],
      "source": [
        "compressed_docs = compression_retriever.get_relevant_documents(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "szdKZBtv1i2U"
      },
      "outputs": [],
      "source": [
        "print(len(compressed_docs))\n",
        "type(compressed_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IE2gMYdq-fon"
      },
      "outputs": [],
      "source": [
        "for i in range(len(compressed_docs)):\n",
        "  print(compressed_docs[i].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ux5vGwes1sc7"
      },
      "outputs": [],
      "source": [
        "output = llm.predict(text=PROMPT.format_prompt(\n",
        "    context=compressed_docs,\n",
        "    question=question\n",
        ").text)\n",
        "\n",
        "type(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQsmJQBSFoK0"
      },
      "outputs": [],
      "source": [
        "question = \"What are arteries and what is their difference with the pulmonary artery\"\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb2.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":8}), llm=llm\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQD_5edmF8Um"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or23Mcow5f4I"
      },
      "source": [
        "#### Advanced RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g5Rq9c2GmeC"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0, model='gpt-4o')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dH5qLuobHPS7"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to Sankrit.\"),\n",
        "    HumanMessage(content=\"What is the capital of India?\"),\n",
        "    AIMessage(content=\"New Delhi\"),\n",
        "    HumanMessage(content=\"How many states are there in India?\")\n",
        "]\n",
        "llm(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIOb5KdUHtB4"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"data\"],\n",
        "    template=''' Give a funny poem, given the user's query and data given below\n",
        "     {data}  '''\n",
        ")\n",
        "\n",
        "system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AXU7moE5LWoG"
      },
      "outputs": [],
      "source": [
        "system_message_prompt.format(data = compressed_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nyoykqp9LZiy"
      },
      "outputs": [],
      "source": [
        "human_template = \"{query}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "O2VfzfvJM7OO"
      },
      "outputs": [],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "chat_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DSlqFVBiNYy_"
      },
      "outputs": [],
      "source": [
        "input_query = input(\"Please input your query: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0E94mcVyNqcz"
      },
      "outputs": [],
      "source": [
        "input_data = retriever_from_llm.get_relevant_documents(query=input_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yrnvwZP0OlpU"
      },
      "outputs": [],
      "source": [
        "print(input_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wTiTSCkANGCa"
      },
      "outputs": [],
      "source": [
        "chat_prompt_with_values = chat_prompt.format_prompt(query=input_query, data=input_data)\n",
        "messages = chat_prompt_with_values.to_messages()\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiJnE3jMPhgs"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0, model='gpt-4o')\n",
        "\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,\n",
        "                                                       base_retriever=vectordb2.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":8}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRo4ZPzkPhgt"
      },
      "outputs": [],
      "source": [
        "input_data = compression_retriever.get_relevant_documents(input_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hvG9xy8oQSwP"
      },
      "outputs": [],
      "source": [
        "chat_prompt_with_values = chat_prompt.format_prompt(query=input_query, data=input_data)\n",
        "messages = chat_prompt_with_values.to_messages()\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "x8ODnusgOrI5"
      },
      "outputs": [],
      "source": [
        "response = llm(messages)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4lB0qtWCO12J"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRNBknG_gOFP"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key, streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)\n",
        "resp = chat(chat_prompt_with_values.to_messages())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-V0f8TzjfkJ"
      },
      "source": [
        "##### Google Search Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU2tSusUkzua"
      },
      "outputs": [],
      "source": [
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BldgunIje_e"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI\n",
        "from google.colab import userdata\n",
        "os.environ['SERPAPI_API_KEY']=userdata.get('SERP_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkaijUpTgRpl"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNJXrkOokPj9"
      },
      "outputs": [],
      "source": [
        "tool_names = [\"serpapi\"]\n",
        "tools = load_tools(tool_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9ftx7N8kQ-j"
      },
      "outputs": [],
      "source": [
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-DJQyTDle46"
      },
      "outputs": [],
      "source": [
        "agent.run(input_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfAyHDL2l4fx"
      },
      "source": [
        "##### Wolfram Alpha Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1SWl2uFmgfC"
      },
      "outputs": [],
      "source": [
        "tool_names = [\"wolfram-alpha\"]\n",
        "tools = load_tools(tool_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXHEcRGYqhHq"
      },
      "source": [
        "#### Ensemble Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEilOX9vrXn9"
      },
      "outputs": [],
      "source": [
        "!pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVl1kwLwqkq4"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrxsixlzqql0"
      },
      "outputs": [],
      "source": [
        "vector_db_bm25 = BM25Retriever.from_documents(documents = split_documents, embedding = embeddings)\n",
        "vector_db_bm25.k = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iU3Xgi5rRse"
      },
      "outputs": [],
      "source": [
        "vectordb_chroma = Chroma.from_documents(documents = split_documents, embedding = embeddings)\n",
        "vectordb_chroma = vectordb_chroma.as_retriever(search_kwargs={\"k\": 4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3nEnqgwsTr5"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever = EnsembleRetriever(retrievers=[vector_db_bm25, vectordb_chroma], weights=[0.5, 0.5], verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1G_r7wdsEKp"
      },
      "outputs": [],
      "source": [
        "ensemble_docs = ensemble_retriever.get_relevant_documents(\"what is the ovary?\")\n",
        "ensemble_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dq8pOAkPH1F"
      },
      "outputs": [],
      "source": []
    }
  ]
}